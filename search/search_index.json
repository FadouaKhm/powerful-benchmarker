{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Powerful Benchmarker \u00b6 Installation \u00b6 pip install powerful-benchmarker Usage \u00b6 Set default flags \u00b6 The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets are located. root_experiment_folder is where you want all experiment data to be saved. Try a basic command \u00b6 The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} (For the rest of this readme, we'll assume the datasets have already been downloaded.) Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-config_dataset.yaml |-config_eval.yaml |-config_general.yaml |-config_loss_and_miners.yaml |-config_models.yaml |-config_optimizers.yaml |-config_transforms.yaml |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Override config options at the command line \u00b6 The default config files use a batch size of 32 . You can override this default value at the command line: python run.py --experiment_name test1 --batch_size 256 Complex options (i.e. nested dictionaries) can also be specified at the command line: python run.py \\ --experiment_name test1 \\ --mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}} The ~OVERRIDE~ suffix is required to completely override complex config options. For example, the following overrides the default loss function : python run.py \\ --experiment_name test1 \\ --loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}} Leave out the ~OVERRIDE~ suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters: python run.py \\ --experiment_name test1 \\ --optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} This will be included along with the default optimizers . We can change the learning rate of the trunk_optimizer, but keep all other parameters the same: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}} Combine yaml files at the command line \u00b6 The following merges the with_cars196 config file into the default config file, in the config_general category. python run.py --experiment_name test1 --config_general [default, with_cars196] This is convenient when you want to change a few settings (specified in with_cars196 ), and keep all the other options unchanged (specified in default ). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify. Resume training \u00b6 The following resumes training for the test1 experiment, using the latest saved models. python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-config_eval.yaml ... |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 . Reproducing benchmark results \u00b6 To reproduce an experiment from the benchmark spreadsheets, use the --reproduce_results flag: 1. In the benchmark spreadsheet, click on the google drive link under the \"config files\" column. 2. Download the folders you want (for example cub200_old_approach_triplet_batch_all ), into some folder on your computer. For example, I downloaded into /home/experiments_to_reproduce 3. Then run: python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\ --experiment_name cub200_old_approach_triplet_batch_all_reproduced If you'd like to change some parameters when reproducing results, you can either make those changes in the config files, or at the command line. For example, maybe you'd like to change the number of dataloaders: python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\ --experiment_name cub200_old_approach_triplet_batch_all_reproduced \\ --dataloader_num_workers 16 \\ --eval_dataloader_num_workers 16 \\ --merge_argparse_when_resuming The merge_argparse_when_resuming flag is required in order to use a different configuration from the one in the reproduce_results folder. Evaluation options \u00b6 By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names. For example --splits_to_eval [train, test] To run evaluation only, use the --evaluate flag. Split schemes and cross validation \u00b6 One weakness of many metric-learning papers is that they have been training and testing on the same handful of datasets for years. They have also been splitting data into a 50/50 train/test split scheme, instead of train/val/test. This has likely lead to overfitting on the \"test\" set, as people have tuned hyperparameters and created algorithms with direct feedback from the \"test\" set. To remedy this situation, this benchmarker allows the user to specify the split scheme. Here's an example config: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 Translation: - The test set consists of classes with labels in [num_labels * test_start_idx, num_labels * (test_start_idx + test_size)] . Note that if we set test_start_idx to 0.9, the range would wrap around to the beginning (0.9 to 1, 0 to 0.4). - The remaining classes will be split into 10 equal sized partitions. - 5 of those partitions will be used for training. In other words, 5-fold cross validation will be performed, but the size of the partitions will be the same as if 10-fold cross validation was being performed. When evaluating the cross-validated models, the best model from each fold will be loaded, and the results be averaged. Alternatively, you can set the config option meta_testing_method to meta_ConcatenateEmbeddings . This will load the best model from each fold, but treat them as one model during evaluation on the test set, by concatenating their outputs. Meta logs \u00b6 When doing cross validation, a new set of meta records will be created. The meta records show the average of the best accuracies of your training runs. You can find these records on tensorboard and in the meta_logs folder. Bayesian optimization to tune hyperparameters \u00b6 You can use bayesian optimization using the same example script . In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. If your parameter operates on a log scale (for example, learning rates), then append ~LOG_BAYESIAN~ . You must also specify the number of iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ {metric_loss: {MultiSimilarityLoss: {alpha~LOG_BAYESIAN~: [0.01, 100], beta~LOG_BAYESIAN~: [0.01, 100], base~BAYESIAN~: [0, 1]}}} \\ --experiment_name cub_bayes_opt \\ If you stop and want to resume bayesian optimization, simply use run.py with the same experiment_name you were using before. You can change the optimization bounds when resuming, by either changing the bounds in your config files or at the command line. If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag. You can also run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\ Register your own classes and modules \u00b6 By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning. Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the example script with this: from pytorch_metric_learning import losses , miners # your custom loss function class YourLossFunction ( losses . BaseMetricLossFunction ): ... # your custom mining function class YourMiningFunction ( miners . BaseTupleMiner ): ... r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"miner\" , YourMiningFunction ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : mining_funcs : tuple_miner : YourMiningFunction : If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . In order to pass this in, the API parser needs to add foo to the trainer kwargs, and this is done in the get_trainer_kwargs method. from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def get_foo ( self ): return \"hello\" def get_trainer_kwargs ( self ): trainer_kwargs = super () . get_trainer_kwargs () trainer_kwargs [ \"foo\" ] = self . get_foo () return trainer_kwargs r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run ()","title":"Home"},{"location":"#powerful-benchmarker","text":"","title":"Powerful Benchmarker"},{"location":"#installation","text":"pip install powerful-benchmarker","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#set-default-flags","text":"The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets are located. root_experiment_folder is where you want all experiment data to be saved.","title":"Set default flags"},{"location":"#try-a-basic-command","text":"The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} (For the rest of this readme, we'll assume the datasets have already been downloaded.) Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-config_dataset.yaml |-config_eval.yaml |-config_general.yaml |-config_loss_and_miners.yaml |-config_models.yaml |-config_optimizers.yaml |-config_transforms.yaml |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs","title":"Try a basic command"},{"location":"#override-config-options-at-the-command-line","text":"The default config files use a batch size of 32 . You can override this default value at the command line: python run.py --experiment_name test1 --batch_size 256 Complex options (i.e. nested dictionaries) can also be specified at the command line: python run.py \\ --experiment_name test1 \\ --mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}} The ~OVERRIDE~ suffix is required to completely override complex config options. For example, the following overrides the default loss function : python run.py \\ --experiment_name test1 \\ --loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}} Leave out the ~OVERRIDE~ suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters: python run.py \\ --experiment_name test1 \\ --optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} This will be included along with the default optimizers . We can change the learning rate of the trunk_optimizer, but keep all other parameters the same: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}}","title":"Override config options at the command line"},{"location":"#combine-yaml-files-at-the-command-line","text":"The following merges the with_cars196 config file into the default config file, in the config_general category. python run.py --experiment_name test1 --config_general [default, with_cars196] This is convenient when you want to change a few settings (specified in with_cars196 ), and keep all the other options unchanged (specified in default ). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.","title":"Combine yaml files at the command line"},{"location":"#resume-training","text":"The following resumes training for the test1 experiment, using the latest saved models. python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-config_eval.yaml ... |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 .","title":"Resume training"},{"location":"#reproducing-benchmark-results","text":"To reproduce an experiment from the benchmark spreadsheets, use the --reproduce_results flag: 1. In the benchmark spreadsheet, click on the google drive link under the \"config files\" column. 2. Download the folders you want (for example cub200_old_approach_triplet_batch_all ), into some folder on your computer. For example, I downloaded into /home/experiments_to_reproduce 3. Then run: python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\ --experiment_name cub200_old_approach_triplet_batch_all_reproduced If you'd like to change some parameters when reproducing results, you can either make those changes in the config files, or at the command line. For example, maybe you'd like to change the number of dataloaders: python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \\ --experiment_name cub200_old_approach_triplet_batch_all_reproduced \\ --dataloader_num_workers 16 \\ --eval_dataloader_num_workers 16 \\ --merge_argparse_when_resuming The merge_argparse_when_resuming flag is required in order to use a different configuration from the one in the reproduce_results folder.","title":"Reproducing benchmark results"},{"location":"#evaluation-options","text":"By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names. For example --splits_to_eval [train, test] To run evaluation only, use the --evaluate flag.","title":"Evaluation options"},{"location":"#split-schemes-and-cross-validation","text":"One weakness of many metric-learning papers is that they have been training and testing on the same handful of datasets for years. They have also been splitting data into a 50/50 train/test split scheme, instead of train/val/test. This has likely lead to overfitting on the \"test\" set, as people have tuned hyperparameters and created algorithms with direct feedback from the \"test\" set. To remedy this situation, this benchmarker allows the user to specify the split scheme. Here's an example config: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 Translation: - The test set consists of classes with labels in [num_labels * test_start_idx, num_labels * (test_start_idx + test_size)] . Note that if we set test_start_idx to 0.9, the range would wrap around to the beginning (0.9 to 1, 0 to 0.4). - The remaining classes will be split into 10 equal sized partitions. - 5 of those partitions will be used for training. In other words, 5-fold cross validation will be performed, but the size of the partitions will be the same as if 10-fold cross validation was being performed. When evaluating the cross-validated models, the best model from each fold will be loaded, and the results be averaged. Alternatively, you can set the config option meta_testing_method to meta_ConcatenateEmbeddings . This will load the best model from each fold, but treat them as one model during evaluation on the test set, by concatenating their outputs.","title":"Split schemes and cross validation"},{"location":"#meta-logs","text":"When doing cross validation, a new set of meta records will be created. The meta records show the average of the best accuracies of your training runs. You can find these records on tensorboard and in the meta_logs folder.","title":"Meta logs"},{"location":"#bayesian-optimization-to-tune-hyperparameters","text":"You can use bayesian optimization using the same example script . In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. If your parameter operates on a log scale (for example, learning rates), then append ~LOG_BAYESIAN~ . You must also specify the number of iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ {metric_loss: {MultiSimilarityLoss: {alpha~LOG_BAYESIAN~: [0.01, 100], beta~LOG_BAYESIAN~: [0.01, 100], base~BAYESIAN~: [0, 1]}}} \\ --experiment_name cub_bayes_opt \\ If you stop and want to resume bayesian optimization, simply use run.py with the same experiment_name you were using before. You can change the optimization bounds when resuming, by either changing the bounds in your config files or at the command line. If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag. You can also run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Bayesian optimization to tune hyperparameters"},{"location":"#register-your-own-classes-and-modules","text":"By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning. Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the example script with this: from pytorch_metric_learning import losses , miners # your custom loss function class YourLossFunction ( losses . BaseMetricLossFunction ): ... # your custom mining function class YourMiningFunction ( miners . BaseTupleMiner ): ... r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"miner\" , YourMiningFunction ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : mining_funcs : tuple_miner : YourMiningFunction : If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . In order to pass this in, the API parser needs to add foo to the trainer kwargs, and this is done in the get_trainer_kwargs method. from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def get_foo ( self ): return \"hello\" def get_trainer_kwargs ( self ): trainer_kwargs = super () . get_trainer_kwargs () trainer_kwargs [ \"foo\" ] = self . get_foo () return trainer_kwargs r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run ()","title":"Register your own classes and modules"},{"location":"api_parsers/","text":"API Parsers \u00b6 BaseAPIParser \u00b6","title":"API Parsers"},{"location":"api_parsers/#api-parsers","text":"","title":"API Parsers"},{"location":"api_parsers/#baseapiparser","text":"","title":"BaseAPIParser"},{"location":"architectures/","text":"Architectures \u00b6 ListOfModels \u00b6 MLP \u00b6","title":"Architectures"},{"location":"architectures/#architectures","text":"","title":"Architectures"},{"location":"architectures/#listofmodels","text":"","title":"ListOfModels"},{"location":"architectures/#mlp","text":"","title":"MLP"},{"location":"datasets/","text":"Datasets \u00b6 Cars196 \u00b6 CUB200 \u00b6 StanfordOnlineProducts \u00b6","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#cars196","text":"","title":"Cars196"},{"location":"datasets/#cub200","text":"","title":"CUB200"},{"location":"datasets/#stanfordonlineproducts","text":"","title":"StanfordOnlineProducts"},{"location":"runners/","text":"Runners \u00b6 BaseRunner \u00b6 BayesOptRunner \u00b6 SingleExperimentRunner \u00b6","title":"Runners"},{"location":"runners/#runners","text":"","title":"Runners"},{"location":"runners/#baserunner","text":"","title":"BaseRunner"},{"location":"runners/#bayesoptrunner","text":"","title":"BayesOptRunner"},{"location":"runners/#singleexperimentrunner","text":"","title":"SingleExperimentRunner"},{"location":"split_managers/","text":"Split Managers \u00b6 BaseSplitManager \u00b6 ClassDisjointSplitManager \u00b6 ClosedSetSplitManager \u00b6 IndexSplitManager \u00b6 PredefinedSplitManager \u00b6 SplitSchemeHolder \u00b6","title":"Split Managers"},{"location":"split_managers/#split-managers","text":"","title":"Split Managers"},{"location":"split_managers/#basesplitmanager","text":"","title":"BaseSplitManager"},{"location":"split_managers/#classdisjointsplitmanager","text":"","title":"ClassDisjointSplitManager"},{"location":"split_managers/#closedsetsplitmanager","text":"","title":"ClosedSetSplitManager"},{"location":"split_managers/#indexsplitmanager","text":"","title":"IndexSplitManager"},{"location":"split_managers/#predefinedsplitmanager","text":"","title":"PredefinedSplitManager"},{"location":"split_managers/#splitschemeholder","text":"","title":"SplitSchemeHolder"},{"location":"utils/","text":"Utils \u00b6 constants \u00b6 dataset_utils \u00b6","title":"Utils"},{"location":"utils/#utils","text":"","title":"Utils"},{"location":"utils/#constants","text":"","title":"constants"},{"location":"utils/#dataset_utils","text":"","title":"dataset_utils"},{"location":"configs/config_dataset/","text":"config_dataset \u00b6 dataset \u00b6 The dataset class and its parameters (if any). Default yaml: dataset : CUB200 : Command line: --dataset { CUB200: {}} splits_to_eval \u00b6 The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Command line: --splits_to_eval [ val ] split_manager \u00b6 The split manager class and its parameters (if any). Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Command line: --split_manager { ClassDisjointSplitManager: { test_size: 0 .5, test_start_idx: 0 .5, num_training_partitions: 4 , num_training_sets: 4 , hierarchy_level: 0 , data_and_label_getter_keys: [ data, label ]}}","title":"Config Dataset"},{"location":"configs/config_dataset/#config_dataset","text":"","title":"config_dataset"},{"location":"configs/config_dataset/#dataset","text":"The dataset class and its parameters (if any). Default yaml: dataset : CUB200 : Command line: --dataset { CUB200: {}}","title":"dataset"},{"location":"configs/config_dataset/#splits_to_eval","text":"The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Command line: --splits_to_eval [ val ]","title":"splits_to_eval"},{"location":"configs/config_dataset/#split_manager","text":"The split manager class and its parameters (if any). Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Command line: --split_manager { ClassDisjointSplitManager: { test_size: 0 .5, test_start_idx: 0 .5, num_training_partitions: 4 , num_training_sets: 4 , hierarchy_level: 0 , data_and_label_getter_keys: [ data, label ]}}","title":"split_manager"},{"location":"configs/config_eval/","text":"config_eval \u00b6","title":"Config Eval"},{"location":"configs/config_eval/#config_eval","text":"","title":"config_eval"},{"location":"configs/config_general/","text":"config_general \u00b6 num_epochs_train \u00b6 The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Command line: --num_epochs_train 1000 iterations_per_epoch \u00b6 If null : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. If an integer, then an epoch consists of this many iterations. Why have this option? For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. Default yaml: iterations_per_epoch : 100 Command line: --iterations_per_epoch 100 save_interval \u00b6 Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Command line: --save_interval 2 check_untrained_accuracy \u00b6 If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Command line: --check_untrained_accuracy True skip_eval_if_already_done \u00b6 If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Command line: --skip_eval_if_already_done True skip_meta_eval_if_already_done \u00b6 The same as skip_eval_if_already_done , but for meta evaluation. Default yaml: skip_meta_eval_if_already_done : True Command line: --skip_meta_eval_if_already_done True","title":"Config General"},{"location":"configs/config_general/#config_general","text":"","title":"config_general"},{"location":"configs/config_general/#num_epochs_train","text":"The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Command line: --num_epochs_train 1000","title":"num_epochs_train"},{"location":"configs/config_general/#iterations_per_epoch","text":"If null : 1 epoch = 1 pass through the dataloader iterator. If sampler=None , then 1 pass through the iterator is 1 pass through the dataset. If you use a sampler, then 1 pass through the iterator is 1 pass through the iterable returned by the sampler. If an integer, then an epoch consists of this many iterations. Why have this option? For samplers like MPerClassSampler or some offline mining method, the iterable returned might be very long or very short etc, and might not be related to the length of the dataset. The length of the epoch might vary each time the sampler creates a new iterable. In these cases, it can be useful to specify iterations_per_epoch so that each \"epoch\" is just a fixed number of iterations. The definition of epoch matters because there's various things like LR schedulers and hooks that depend on an epoch ending. Default yaml: iterations_per_epoch : 100 Command line: --iterations_per_epoch 100","title":"iterations_per_epoch"},{"location":"configs/config_general/#save_interval","text":"Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Command line: --save_interval 2","title":"save_interval"},{"location":"configs/config_general/#check_untrained_accuracy","text":"If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Command line: --check_untrained_accuracy True","title":"check_untrained_accuracy"},{"location":"configs/config_general/#skip_eval_if_already_done","text":"If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Command line: --skip_eval_if_already_done True","title":"skip_eval_if_already_done"},{"location":"configs/config_general/#skip_meta_eval_if_already_done","text":"The same as skip_eval_if_already_done , but for meta evaluation. Default yaml: skip_meta_eval_if_already_done : True Command line: --skip_meta_eval_if_already_done True","title":"skip_meta_eval_if_already_done"},{"location":"configs/config_loss_and_miners/","text":"config_loss_and_miners \u00b6 loss_funcs \u00b6 An object mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Command line: --loss_funcs { metric_loss: { ContrastiveLoss: {}}} sampler \u00b6 The sampler class, or null if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Command line: --sampler { MPerClassSampler: { m: 4 }} mining_funcs \u00b6 An object mapping from strings to mining classes. The strings should match the mining names used by your trainer. Default yaml: mining_funcs : {} A non-empty yaml example: mining_funcs : tuple_miner : MultiSimilarityMiner : epsilon : 0.1 Command line: --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"Config Loss and Miners"},{"location":"configs/config_loss_and_miners/#config_loss_and_miners","text":"","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#loss_funcs","text":"An object mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Command line: --loss_funcs { metric_loss: { ContrastiveLoss: {}}}","title":"loss_funcs"},{"location":"configs/config_loss_and_miners/#sampler","text":"The sampler class, or null if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Command line: --sampler { MPerClassSampler: { m: 4 }}","title":"sampler"},{"location":"configs/config_loss_and_miners/#mining_funcs","text":"An object mapping from strings to mining classes. The strings should match the mining names used by your trainer. Default yaml: mining_funcs : {} A non-empty yaml example: mining_funcs : tuple_miner : MultiSimilarityMiner : epsilon : 0.1 Command line: --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"mining_funcs"},{"location":"configs/config_models/","text":"config_models \u00b6 models \u00b6 An object mapping from strings to the models that create embeddings. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Command line: --models { trunk: { bninception: { pretrained: imagenet }} , embedder: { MLP: { layer_size: [ 128 ]}}}","title":"Config Models"},{"location":"configs/config_models/#config_models","text":"","title":"config_models"},{"location":"configs/config_models/#models","text":"An object mapping from strings to the models that create embeddings. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Command line: --models { trunk: { bninception: { pretrained: imagenet }} , embedder: { MLP: { layer_size: [ 128 ]}}}","title":"models"},{"location":"configs/config_optimizers/","text":"config_optimizers \u00b6 optimizers \u00b6 An object mapping from strings to optimizer objects. The strings should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Command line: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .000001, weight_decay: 0 .0001, momentum: 0 .9 }} , embedder_optimizer: { RMSprop: { lr: 0 .000001, weight_decay: 0 .0001, momentum: 0 .9 }}}","title":"Config Optimizers"},{"location":"configs/config_optimizers/#config_optimizers","text":"","title":"config_optimizers"},{"location":"configs/config_optimizers/#optimizers","text":"An object mapping from strings to optimizer objects. The strings should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Command line: --optimizers { trunk_optimizer: { RMSprop: { lr: 0 .000001, weight_decay: 0 .0001, momentum: 0 .9 }} , embedder_optimizer: { RMSprop: { lr: 0 .000001, weight_decay: 0 .0001, momentum: 0 .9 }}}","title":"optimizers"},{"location":"configs/config_transforms/","text":"config_transforms \u00b6 transforms \u00b6 Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Command line: --transforms { train: { Resize: { size: 256 } , RandomResizedCrop: { scale: [ 0 .16, 1 ] , ratio: [ 0 .75, 1 .33 ] , size: 227 } , RandomHorizontalFlip: { p: 0 .5 }} , eval: { Resize: { size: 256 } , CenterCrop: { size: 227 }}}","title":"Config Transforms"},{"location":"configs/config_transforms/#config_transforms","text":"","title":"config_transforms"},{"location":"configs/config_transforms/#transforms","text":"Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Command line: --transforms { train: { Resize: { size: 256 } , RandomResizedCrop: { scale: [ 0 .16, 1 ] , ratio: [ 0 .75, 1 .33 ] , size: 227 } , RandomHorizontalFlip: { p: 0 .5 }} , eval: { Resize: { size: 256 } , CenterCrop: { size: 227 }}}","title":"transforms"},{"location":"papers/mlrc/","text":"A Metric Learning Reality Check \u00b6 This page contains additional information for the ECCV 2020 paper by Musgrave et al. Frequently Asked Questions \u00b6 Isn't it unfair to fix the model, optimizer, learning rate, and embedding size? \u00b6 Why weren't more hard-mining methods evaluated? \u00b6 Why was the batch size set to 32 for most of the results? \u00b6 What is the difference between MAP@R and MAP? \u00b6 For the contrastive loss, why is the optimal positive margin a negative value? \u00b6 A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary. Optimization plots \u00b6","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#a-metric-learning-reality-check","text":"This page contains additional information for the ECCV 2020 paper by Musgrave et al.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"papers/mlrc/#isnt-it-unfair-to-fix-the-model-optimizer-learning-rate-and-embedding-size","text":"","title":"Isn't it unfair to fix the model, optimizer, learning rate, and embedding size?"},{"location":"papers/mlrc/#why-werent-more-hard-mining-methods-evaluated","text":"","title":"Why weren't more hard-mining methods evaluated?"},{"location":"papers/mlrc/#why-was-the-batch-size-set-to-32-for-most-of-the-results","text":"","title":"Why was the batch size set to 32 for most of the results?"},{"location":"papers/mlrc/#what-is-the-difference-between-mapr-and-map","text":"","title":"What is the difference between MAP@R and MAP?"},{"location":"papers/mlrc/#for-the-contrastive-loss-why-is-the-optimal-positive-margin-a-negative-value","text":"A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary.","title":"For the contrastive loss, why is the optimal positive margin a negative value?"},{"location":"papers/mlrc/#optimization-plots","text":"","title":"Optimization plots"}]}