{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Powerful Benchmarker \u00b6 Installation \u00b6 pip install powerful-benchmarker Getting started \u00b6 Set default flags \u00b6 The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved. Try a basic command \u00b6 The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} If the code runs properly, you'll see training and testing progress like this: Experiment folder format \u00b6 Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format. View experiment data \u00b6 There are multiple ways to view experiment data: Tensorboard \u00b6 Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this: CSV \u00b6 Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders. SQLite \u00b6 Use DB Browser to open the database files that are saved in the saved_csvs folders. Resume training \u00b6 You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best Keep track of changes \u00b6 Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 . Reproduce an experiment \u00b6 To reproduce an experiment, use the --reproduce_results flag. For example, here's how to reproduce the experiments in the benchmark spreadsheets : In the spreadsheet, find the experiment you want to reproduce, click on its google drive link under the \"config files\" column, and download the folder. Run: python run.py --reproduce_results <the_downloaded_folder> \\ --experiment_name <experiment_name> Evaluating on specific splits \u00b6 By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag. Cross validation split schemes \u00b6 In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint. Advanced usage \u00b6 Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Home"},{"location":"#powerful-benchmarker","text":"","title":"Powerful Benchmarker"},{"location":"#installation","text":"pip install powerful-benchmarker","title":"Installation"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#set-default-flags","text":"The easiest way to get started is to download the example script . Then change the default values for the following flags: pytorch_home is where you want to save downloaded pretrained models. dataset_root is where your datasets will be downloaded, or where they are already located. root_experiment_folder is where you want all experiment data to be saved.","title":"Set default flags"},{"location":"#try-a-basic-command","text":"The following command will run an experiment using the default config files , as well as download the CUB200 dataset into your dataset_root python run.py --experiment_name test1 --dataset {CUB200: {download: True}} If the code runs properly, you'll see training and testing progress like this:","title":"Try a basic command"},{"location":"#experiment-folder-format","text":"Experiment data is saved in the following format: <root_experiment_folder> |-<experiment_name> |-configs |-<split scheme name> |-saved_models |-saved_csvs |-tensorboard_logs |-meta_logs |-saved_csvs |-tensorboard_logs Here's what's in each subfolder: configs contains the yaml config files necessary to reproduce the experiment. <split scheme name>/save_models contains saved pytorch models for a particular split scheme. (A split scheme simply refers to the way train/val/test splits are formed.) <split scheme name>/saved_csvs contains CSV files with data collected during training. It also contains an SQLite database file with the same data. <split scheme name>/tensorboard_logs contains the same information in <split scheme name>/saved_csvs , but in tensorboard format. meta_logs/saved_csvs contains CSV files for aggregate and ensemble accuracies. It also contains an SQLite database file with the same data. meta_logs/tensorboard contains the same information in meta_logs/save_csvs , but in tensorboard format.","title":"Experiment folder format"},{"location":"#view-experiment-data","text":"There are multiple ways to view experiment data:","title":"View experiment data"},{"location":"#tensorboard","text":"Go to the <experiment_name> folder, and run tensorboard at the command line: tensorboard --logdir = . --port = 12345 Then in your web browser, go to localhost:<port> , where <port> is specified in the tensorboard command. You'll see plots like this:","title":"Tensorboard"},{"location":"#csv","text":"Use any text editor or spreadsheet program to view the csv files that are saved in the saved_csvs folders.","title":"CSV"},{"location":"#sqlite","text":"Use DB Browser to open the database files that are saved in the saved_csvs folders.","title":"SQLite"},{"location":"#resume-training","text":"You can interrupt the program and resume training at a later time: python run.py --experiment_name test1 --resume_training latest You can also resume using the model with the best validation accuracy: python run.py --experiment_name test1 --resume_training best","title":"Resume training"},{"location":"#keep-track-of-changes","text":"Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run: python run.py --experiment_name test1 --resume_training latest \\ --num_epochs_train 150 --merge_argparse_when_resuming (The merge_argparse_when_resuming flag tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.) Now in your experiments folder you'll see the original config files, and a new folder starting with resume_training . <root_experiment_folder> |-<experiment_name> |-configs |-resume_training_config_diffs_<underscore delimited numbers> ... This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file config_general.yaml with a single line: num_epochs_train: 150 . The underscore delimited numbers in the folder name indicate which models were loaded for each split scheme . For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named resume_training_config_diffs_50_30_0 .","title":"Keep track of changes"},{"location":"#reproduce-an-experiment","text":"To reproduce an experiment, use the --reproduce_results flag. For example, here's how to reproduce the experiments in the benchmark spreadsheets : In the spreadsheet, find the experiment you want to reproduce, click on its google drive link under the \"config files\" column, and download the folder. Run: python run.py --reproduce_results <the_downloaded_folder> \\ --experiment_name <experiment_name>","title":"Reproduce an experiment"},{"location":"#evaluating-on-specific-splits","text":"By default, your model will be saved and evaluated on the validation set every save_interval epochs. To get accuracy for specific splits, use the --splits_to_eval flag and pass in a python-style list of split names: --splits_to_eval [train, test] . To run evaluation only, use the --evaluate or --evaluate_ensemble flag.","title":"Evaluating on specific splits"},{"location":"#cross-validation-split-schemes","text":"In this library, splits are not hard-coded into the dataset classes. Instead, train/val/test splits are created by a SplitManager , as specified in the config_dataset file: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 This particular configuration will set aside the second 50% of classes for the test set. Then the first 50% of classes will be used for 4-fold cross validation, in which the train and val splits are always class-disjoint.","title":"Cross validation split schemes"},{"location":"#advanced-usage","text":"Here are some other important features of this library: The powerful command line syntax that allows you to easily override, modify, merge, and delete config options. Easy and flexible hyperparameter optimization The ability to add custom modules , without having to delve into the benchmarking code.","title":"Advanced usage"},{"location":"custom/","text":"Adding Custom Modules \u00b6 Register your own classes and modules \u00b6 By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning. Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the example script with this: from pytorch_metric_learning import losses , miners # your custom loss function class YourLossFunction ( losses . BaseMetricLossFunction ): ... # your custom mining function class YourMiningFunction ( miners . BaseTupleMiner ): ... r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"miner\" , YourMiningFunction ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : mining_funcs : tuple_miner : YourMiningFunction : If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . In order to pass this in, the API parser needs to add foo to the trainer kwargs, and this is done in the get_trainer_kwargs method. from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def get_foo ( self ): return \"hello\" def get_trainer_kwargs ( self ): trainer_kwargs = super () . get_trainer_kwargs () trainer_kwargs [ \"foo\" ] = self . get_foo () return trainer_kwargs r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run ()","title":"Adding Custom Modules"},{"location":"custom/#adding-custom-modules","text":"","title":"Adding Custom Modules"},{"location":"custom/#register-your-own-classes-and-modules","text":"By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning. Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the example script with this: from pytorch_metric_learning import losses , miners # your custom loss function class YourLossFunction ( losses . BaseMetricLossFunction ): ... # your custom mining function class YourMiningFunction ( miners . BaseTupleMiner ): ... r = runner ( ** ( args . __dict__ )) # make the runner aware of them r . register ( \"loss\" , YourLossFunction ) r . register ( \"miner\" , YourMiningFunction ) r . run () Now you can access your custom classes just like any other class: loss_funcs : metric_loss : YourLossFunction : mining_funcs : tuple_miner : YourMiningFunction : If you have a module containing multiple classes and you want to register all those classes, you can simply register the module: import YourModuleOfLosses r . register ( \"loss\" , YourModuleOfLosses ) Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be APIParser<name of your training method> . Here's an example where I make a trainer that extends trainers.MetricLossOnly , and takes in an additional argument foo . In order to pass this in, the API parser needs to add foo to the trainer kwargs, and this is done in the get_trainer_kwargs method. from pytorch_metric_learning import trainers from powerful_benchmarker import api_parsers class YourTrainer ( trainers . MetricLossOnly ): def __init__ ( self , foo , ** kwargs ): super () . __init__ ( ** kwargs ) self . foo = foo print ( \"foo = \" , self . foo ) class APIYourTrainer ( api_parsers . BaseAPIParser ): def get_foo ( self ): return \"hello\" def get_trainer_kwargs ( self ): trainer_kwargs = super () . get_trainer_kwargs () trainer_kwargs [ \"foo\" ] = self . get_foo () return trainer_kwargs r = runner ( ** ( args . __dict__ )) r . register ( \"trainer\" , YourTrainer ) r . register ( \"api_parser\" , APIYourTrainer ) r . run ()","title":"Register your own classes and modules"},{"location":"hyperparams/","text":"Tuning Hyperparameters \u00b6 Bayesian optimization \u00b6 Syntax \u00b6 To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ { metric_loss: { MultiSimilarityLoss: { alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\ Resume optimization \u00b6 If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before. Change optimization bounds \u00b6 You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag. Run reproductions \u00b6 You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Hyperparameter Optimization"},{"location":"hyperparams/#tuning-hyperparameters","text":"","title":"Tuning Hyperparameters"},{"location":"hyperparams/#bayesian-optimization","text":"","title":"Bayesian optimization"},{"location":"hyperparams/#syntax","text":"To tune hyperparameters using bayesian optimization: In your config files or at the command line, append ~BAYESIAN~ to any parameter that you want to tune, followed by a lower and upper bound in square brackets. Use ~LOG_BAYESIAN~ for log-scaled parameters, and ~INT_BAYESIAN~ for integer parameters. Specify the number of bayesian optimization iterations with the --bayes_opt_iters command line flag. Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss. python run.py --bayes_opt_iters 50 \\ --loss_funcs~OVERRIDE~ { metric_loss: { MultiSimilarityLoss: { alpha~LOG_BAYESIAN~: [ 0 .01, 100 ] , beta~LOG_BAYESIAN~: [ 0 .01, 100 ] , base~BAYESIAN~: [ 0 , 1 ]}}} \\ --experiment_name cub_bayes_opt \\","title":"Syntax"},{"location":"hyperparams/#resume-optimization","text":"If you stop and want to resume bayesian optimization, simply run run.py with the same experiment_name you were using before.","title":"Resume optimization"},{"location":"hyperparams/#change-optimization-bounds","text":"You can change the optimization bounds when resuming, by either changing the bounds in the config files or at the command line. The command line is preferable, because any config diffs will be recorded (just like in regular experiments ). If you're using the command line, make sure to also use the --merge_argparse_when_resuming flag.","title":"Change optimization bounds"},{"location":"hyperparams/#run-reproductions","text":"You can run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the reproductions flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization. python run.py --bayes_opt_iters 50 --reproductions 10 \\ --experiment_name cub_bayes_opt \\","title":"Run reproductions"},{"location":"syntax/","text":"Command Line Syntax \u00b6 Override config options at the command line \u00b6 The default config files use a batch size of 32 . You can override this default value at the command line: python run.py --experiment_name test1 --batch_size 256 Complex options (i.e. nested dictionaries) can also be specified at the command line: python run.py \\ --experiment_name test1 \\ --mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}} The ~OVERRIDE~ suffix is required to completely override complex config options. For example, the following overrides the default loss function : python run.py \\ --experiment_name test1 \\ --loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}} Leave out the ~OVERRIDE~ suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters: python run.py \\ --experiment_name test1 \\ --optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} This will be included along with the default optimizers . We can change the learning rate of the trunk_optimizer, but keep all other parameters the same: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}} Combine yaml files at the command line \u00b6 The following merges the with_cars196 config file into the default config file, in the config_general category. python run.py --experiment_name test1 --config_general [default, with_cars196] This is convenient when you want to change a few settings (specified in with_cars196 ), and keep all the other options unchanged (specified in default ). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.","title":"Command Line Syntax"},{"location":"syntax/#command-line-syntax","text":"","title":"Command Line Syntax"},{"location":"syntax/#override-config-options-at-the-command-line","text":"The default config files use a batch size of 32 . You can override this default value at the command line: python run.py --experiment_name test1 --batch_size 256 Complex options (i.e. nested dictionaries) can also be specified at the command line: python run.py \\ --experiment_name test1 \\ --mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}} The ~OVERRIDE~ suffix is required to completely override complex config options. For example, the following overrides the default loss function : python run.py \\ --experiment_name test1 \\ --loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}} Leave out the ~OVERRIDE~ suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters: python run.py \\ --experiment_name test1 \\ --optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} This will be included along with the default optimizers . We can change the learning rate of the trunk_optimizer, but keep all other parameters the same: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: python run.py \\ --experiment_name test1 \\ --optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}}","title":"Override config options at the command line"},{"location":"syntax/#combine-yaml-files-at-the-command-line","text":"The following merges the with_cars196 config file into the default config file, in the config_general category. python run.py --experiment_name test1 --config_general [default, with_cars196] This is convenient when you want to change a few settings (specified in with_cars196 ), and keep all the other options unchanged (specified in default ). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.","title":"Combine yaml files at the command line"},{"location":"code/aggregators/","text":"Aggregators \u00b6 MeanAggregator \u00b6","title":"Aggregators"},{"location":"code/aggregators/#aggregators","text":"","title":"Aggregators"},{"location":"code/aggregators/#meanaggregator","text":"","title":"MeanAggregator"},{"location":"code/api_parsers/","text":"API Parsers \u00b6 BaseAPIParser \u00b6","title":"API Parsers"},{"location":"code/api_parsers/#api-parsers","text":"","title":"API Parsers"},{"location":"code/api_parsers/#baseapiparser","text":"","title":"BaseAPIParser"},{"location":"code/architectures/","text":"Architectures \u00b6 ListOfModels \u00b6 MLP \u00b6","title":"Architectures"},{"location":"code/architectures/#architectures","text":"","title":"Architectures"},{"location":"code/architectures/#listofmodels","text":"","title":"ListOfModels"},{"location":"code/architectures/#mlp","text":"","title":"MLP"},{"location":"code/datasets/","text":"Datasets \u00b6 Cars196 \u00b6 CUB200 \u00b6 StanfordOnlineProducts \u00b6","title":"Datasets"},{"location":"code/datasets/#datasets","text":"","title":"Datasets"},{"location":"code/datasets/#cars196","text":"","title":"Cars196"},{"location":"code/datasets/#cub200","text":"","title":"CUB200"},{"location":"code/datasets/#stanfordonlineproducts","text":"","title":"StanfordOnlineProducts"},{"location":"code/ensembles/","text":"Ensembles \u00b6 ConcatenateEmbeddings \u00b6","title":"Ensembles"},{"location":"code/ensembles/#ensembles","text":"","title":"Ensembles"},{"location":"code/ensembles/#concatenateembeddings","text":"","title":"ConcatenateEmbeddings"},{"location":"code/factories/","text":"Factories \u00b6","title":"Factories"},{"location":"code/factories/#factories","text":"","title":"Factories"},{"location":"code/runners/","text":"Runners \u00b6 BaseRunner \u00b6 BayesOptRunner \u00b6 SingleExperimentRunner \u00b6","title":"Runners"},{"location":"code/runners/#runners","text":"","title":"Runners"},{"location":"code/runners/#baserunner","text":"","title":"BaseRunner"},{"location":"code/runners/#bayesoptrunner","text":"","title":"BayesOptRunner"},{"location":"code/runners/#singleexperimentrunner","text":"","title":"SingleExperimentRunner"},{"location":"code/split_managers/","text":"Split Managers \u00b6 BaseSplitManager \u00b6 ClassDisjointSplitManager \u00b6 ClosedSetSplitManager \u00b6 IndexSplitManager \u00b6 PredefinedSplitManager \u00b6 SplitSchemeHolder \u00b6","title":"Split Managers"},{"location":"code/split_managers/#split-managers","text":"","title":"Split Managers"},{"location":"code/split_managers/#basesplitmanager","text":"","title":"BaseSplitManager"},{"location":"code/split_managers/#classdisjointsplitmanager","text":"","title":"ClassDisjointSplitManager"},{"location":"code/split_managers/#closedsetsplitmanager","text":"","title":"ClosedSetSplitManager"},{"location":"code/split_managers/#indexsplitmanager","text":"","title":"IndexSplitManager"},{"location":"code/split_managers/#predefinedsplitmanager","text":"","title":"PredefinedSplitManager"},{"location":"code/split_managers/#splitschemeholder","text":"","title":"SplitSchemeHolder"},{"location":"code/utils/","text":"Utils \u00b6 constants \u00b6 dataset_utils \u00b6","title":"Utils"},{"location":"code/utils/#utils","text":"","title":"Utils"},{"location":"code/utils/#constants","text":"","title":"constants"},{"location":"code/utils/#dataset_utils","text":"","title":"dataset_utils"},{"location":"configs/config_dataset/","text":"config_dataset \u00b6 dataset \u00b6 This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}} splits_to_eval \u00b6 The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ] split_manager \u00b6 The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"config_dataset"},{"location":"configs/config_dataset/#config_dataset","text":"","title":"config_dataset"},{"location":"configs/config_dataset/#dataset","text":"This is the dataset that will be used for training, validation, and testing. Default yaml: dataset : CUB200 : Example command line modification: # Change dataset to Cars196 --dataset~OVERRIDE~ { Cars196: {}}","title":"dataset"},{"location":"configs/config_dataset/#splits_to_eval","text":"The names of splits for which accuracy should be computed. Default yaml: splits_to_eval : - val Example command line modification: # Eval on train, val, and test. --splits_to_eval [ train, val, test ]","title":"splits_to_eval"},{"location":"configs/config_dataset/#split_manager","text":"The split manager determines how the train/val/test splits are formed. Default yaml: split_manager : ClassDisjointSplitManager : test_size : 0.5 test_start_idx : 0.5 num_training_partitions : 4 num_training_sets : 4 hierarchy_level : 0 data_and_label_getter_keys : [ data , label ] Example command line modification: # Change number of training sets to 2, and the test size to 0.3 --split_manager~APPLY~2 { test_size: 0 .3, num_training_sets: 2 }","title":"split_manager"},{"location":"configs/config_eval/","text":"config_eval \u00b6 tester \u00b6 The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False } aggregator \u00b6 The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}} ensemble \u00b6 The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False hook_container \u00b6 The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"config_eval"},{"location":"configs/config_eval/#config_eval","text":"","title":"config_eval"},{"location":"configs/config_eval/#tester","text":"The tester computes the accuracy of your model. Default yaml: tester : GlobalEmbeddingSpaceTester : reference_set : compared_to_self normalize_embeddings : True use_trunk_output : False batch_size : 32 dataloader_num_workers : 2 pca : null accuracy_calculator : AccuracyCalculator : label_hierarchy_level : 0 Example command line modification: # Change batch size to 256 and don't normalize embeddings --tester~APPLY~2 { batch_size: 256 , normalize_embeddings: False }","title":"tester"},{"location":"configs/config_eval/#aggregator","text":"The aggregator takes the accuracies from all the cross-validation models, and returns a single number to represent the overall performance. Default yaml: aggregator : MeanAggregator : Example command line modification: # Use your own custom aggregator --aggregator~OVERRIDE~ { YourCustomAggregator: {}}","title":"aggregator"},{"location":"configs/config_eval/#ensemble","text":"The ensemble combines the cross-validation models into a single model. Default yaml: ensemble : ConcatenateEmbeddings : normalize_embeddings : True use_trunk_output : False","title":"ensemble"},{"location":"configs/config_eval/#hook_container","text":"The hook container contains end-of-testing, end-of-epoch, and end-of-iteration hooks. It also contains a record keeper, for writing and reading to database files. Default yaml: hook_container : HookContainer : primary_metric : mean_average_precision_at_r validation_split_name : val Example command line modification: # Change the primary metric to precision_at_1 --hook_container~APPLY~2 { primary_metric: precision_at_1 }","title":"hook_container"},{"location":"configs/config_factories/","text":"config_factories \u00b6 factories \u00b6 Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"config_factories"},{"location":"configs/config_factories/#config_factories","text":"","title":"config_factories"},{"location":"configs/config_factories/#factories","text":"Factories determine how objects are constructed, based on parameters in the config files, and parameters generated within the code. Default yaml: factories : model : ModelFactory : {} loss : LossFactory : {} miner : MinerFactory : {} sampler : SamplerFactory : {} optimizer : OptimizerFactory : {} tester : TesterFactory : {} trainer : TrainerFactory : {} transform : TransformFactory : {} split_manager : SplitManagerFactory : {} record_keeper : RecordKeeperFactory : {} hook : HookFactory : {} aggregator : AggregatorFactory : {} ensemble : EnsembleFactory : {} Example command line modification: # Set the base_output_model_size manually --factories { model~APPLY~2: { base_output_model_size: 1024 }}","title":"factories"},{"location":"configs/config_general/","text":"config_general \u00b6 trainer \u00b6 The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null } num_epochs_train \u00b6 The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100 save_interval \u00b6 Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10 patience \u00b6 Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null check_untrained_accuracy \u00b6 If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False skip_eval_if_already_done \u00b6 If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False skip_ensemble_eval_if_already_done \u00b6 The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False save_figures_on_tensorboard \u00b6 Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True save_lists_in_db \u00b6 In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True override_required_compatible_factories \u00b6 Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"config_general"},{"location":"configs/config_general/#config_general","text":"","title":"config_general"},{"location":"configs/config_general/#trainer","text":"The trainer trains your model. Default yaml: trainer : MetricLossOnly : iterations_per_epoch : 100 dataloader_num_workers : 2 batch_size : 32 freeze_trunk_batchnorm : True label_hierarchy_level : 0 loss_weights : null set_min_label_to_zero : True Example command line modification: # Swap in a different trainer, but keep the input parameters the same --trainer~SWAP~1 { CascadedEmbeddings: null }","title":"trainer"},{"location":"configs/config_general/#num_epochs_train","text":"The maximum number of epochs to train for. Default yaml: num_epochs_train : 1000 Example command line modification: --num_epochs_train 100","title":"num_epochs_train"},{"location":"configs/config_general/#save_interval","text":"Models will be evaluated and saved every save_interval epochs. Default yaml: save_interval : 2 Example command line modification: --save_interval 10","title":"save_interval"},{"location":"configs/config_general/#patience","text":"Training will end if the validation accuracy stops improving after patience+1 epochs. Default yaml: save_interval : 2 Example command line modification: # Don't use patience at all --patience null","title":"patience"},{"location":"configs/config_general/#check_untrained_accuracy","text":"If True , then the tester will compute accuracy for the initial trunk (epoch -1) and initial trunk + embedder (epoch 0). Otherwise, these will be skipped. Default yaml: check_untrained_accuracy : True Example command line modification: --check_untrained_accuracy False","title":"check_untrained_accuracy"},{"location":"configs/config_general/#skip_eval_if_already_done","text":"If True , then the tester will skip evaluation if a split/epoch has already been logged in the log files. If False , then the tester will evaluate a split/epoch regardless of whether it has already been done in the past. Previous logs will be preserved, hence the logs will contain duplicate results, and the most recent version for any split/epoch will be considered the \"official\" value for that split/epoch. Default yaml: skip_eval_if_already_done : True Example command line modification: --skip_eval_if_already_done False","title":"skip_eval_if_already_done"},{"location":"configs/config_general/#skip_ensemble_eval_if_already_done","text":"The same as skip_eval_if_already_done , but for ensembles. Default yaml: skip_ensemble_eval_if_already_done : True Example command line modification: --skip_ensemble_eval_if_already_done False","title":"skip_ensemble_eval_if_already_done"},{"location":"configs/config_general/#save_figures_on_tensorboard","text":"Use matplotlib to plot things on tensorboard. (Most data doesn't require matplotlib.) Default yaml: save_figures_on_tensorboard : False Example command line modification: --save_figures_on_tensorboard True","title":"save_figures_on_tensorboard"},{"location":"configs/config_general/#save_lists_in_db","text":"In record-keeper, non-scalar values are saved in the database as json-lists. This setting is False by default, because these lists can sometimes be quite large, causing the database file size to grow quickly. Default yaml: save_lists_in_db : False Example command line modification: --save_lists_in_db True","title":"save_lists_in_db"},{"location":"configs/config_general/#override_required_compatible_factories","text":"Each APIParser comes with predefined compatible factories, which are used by default, regardless of what is specified in the factories config option. This allows you to specify a trainer without having to specify all the required factories. However, if you have your own custom factory that you know is compatible, and want to use that instead, you should set this flag to True. Default yaml: override_required_compatible_factories : False Example command line modification: --override_required_compatible_factories True","title":"override_required_compatible_factories"},{"location":"configs/config_loss_and_miners/","text":"config_loss_and_miners \u00b6 loss_funcs \u00b6 The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}} sampler \u00b6 The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {} mining_funcs \u00b6 Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#config_loss_and_miners","text":"","title":"config_loss_and_miners"},{"location":"configs/config_loss_and_miners/#loss_funcs","text":"The loss functions are given embeddings and labels, and output a value on which back propagation can be performed. This config option is a mapping from strings to loss classes. The strings should match the loss names used by your trainer. Default yaml: loss_funcs : metric_loss : ContrastiveLoss : Example command line modification: # Use a different loss function --loss_funcs { metric_loss~OVERRIDE~: { MultiSimilarityLoss: { alpha: 0 .1, beta: 40 , base: 0 .5 }}}","title":"loss_funcs"},{"location":"configs/config_loss_and_miners/#sampler","text":"The sampler is passed to the PyTorch dataloader, and determines how batches are formed. Use {} if you want random sampling. Default yaml: sampler : MPerClassSampler : m : 4 Example command line modification: # Use random sampling --sampler~OVERRIDE~ {}","title":"sampler"},{"location":"configs/config_loss_and_miners/#mining_funcs","text":"Mining functions determine the best tuples to train on, within an arbitrarily formed batch. This config option is a mapping from strings to miner classes. The strings should match the miner names used by your trainer. Default yaml: mining_funcs : {} Example command line modification: # Use a miner --mining_funcs { tuple_miner: { MultiSimilarityMiner: { epsilon: 0 .1 }}}","title":"mining_funcs"},{"location":"configs/config_models/","text":"config_models \u00b6 models \u00b6 The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"config_models"},{"location":"configs/config_models/#config_models","text":"","title":"config_models"},{"location":"configs/config_models/#models","text":"The models take in input (like images, text etc.) and output embeddings. There is no specific requires about what the structure of the trunk and embedder. The only requirement is that the trunk's output can be fed into the embedder. For example, if you want to use the bninception model, but don't want to append any layers after it, you can set embedder to Identity . This will make the embedder's output equal to its input. Default yaml: models : trunk : bninception : pretrained : imagenet embedder : MLP : layer_sizes : - 128 Example command line modification: # Set embedder to Identity. --models { embedder~OVERRIDE~: { Identity: {}}} \\ # You'll need to delete the embedder_optimizer, because Identity() has no parameters --optimizers { embedder_optimizer~DELETE~: null }","title":"models"},{"location":"configs/config_optimizers/","text":"config_optimizers \u00b6 optimizers \u00b6 Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"config_optimizers"},{"location":"configs/config_optimizers/#config_optimizers","text":"","title":"config_optimizers"},{"location":"configs/config_optimizers/#optimizers","text":"Optimizers determine how your model weights are updated. This config option maps from strings to optimizer classes. Each string should have the form <model_name>_optimizer . Default yaml: optimizers : trunk_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 embedder_optimizer : RMSprop : lr : 0.000001 weight_decay : 0.0001 momentum : 0.9 Example command line modification: # Change the learning rate to 0.01, for both the trunk and embedder --optimizers~APPLY~3 { lr: 0 .01 }","title":"optimizers"},{"location":"configs/config_transforms/","text":"config_transforms \u00b6 transforms \u00b6 Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"config_transforms"},{"location":"configs/config_transforms/#config_transforms","text":"","title":"config_transforms"},{"location":"configs/config_transforms/#transforms","text":"Specifies the transforms to be used during training and during evaluation. ToTensor() and Normalize do not need to be specified, as they are added by default. Default yaml: transforms : train : Resize : size : 256 RandomResizedCrop : scale : 0.16 1 ratio : 0.75 1.33 size : 227 RandomHorizontalFlip : p : 0.5 eval : Resize : size : 256 CenterCrop : size : 227 Example command line modification: # Use RandomVerticalFlip instead of RandomHorizontalFlip --transforms { train~SWAP~1: { RandomHorizontalFlip: RandomVerticalFlip }}","title":"transforms"},{"location":"papers/mlrc/","text":"A Metric Learning Reality Check \u00b6 This page contains additional information for the ECCV 2020 paper by Musgrave et al. Frequently Asked Questions \u00b6 Isn't it unfair to fix the model, optimizer, learning rate, and embedding size? \u00b6 Why weren't more hard-mining methods evaluated? \u00b6 Why was the batch size set to 32 for most of the results? \u00b6 What is the difference between MAP@R and MAP? \u00b6 For the contrastive loss, why is the optimal positive margin a negative value? \u00b6 A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary. Optimization plots \u00b6","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#a-metric-learning-reality-check","text":"This page contains additional information for the ECCV 2020 paper by Musgrave et al.","title":"A Metric Learning Reality Check"},{"location":"papers/mlrc/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"papers/mlrc/#isnt-it-unfair-to-fix-the-model-optimizer-learning-rate-and-embedding-size","text":"","title":"Isn't it unfair to fix the model, optimizer, learning rate, and embedding size?"},{"location":"papers/mlrc/#why-werent-more-hard-mining-methods-evaluated","text":"","title":"Why weren't more hard-mining methods evaluated?"},{"location":"papers/mlrc/#why-was-the-batch-size-set-to-32-for-most-of-the-results","text":"","title":"Why was the batch size set to 32 for most of the results?"},{"location":"papers/mlrc/#what-is-the-difference-between-mapr-and-map","text":"","title":"What is the difference between MAP@R and MAP?"},{"location":"papers/mlrc/#for-the-contrastive-loss-why-is-the-optimal-positive-margin-a-negative-value","text":"A negative value should be equivalent to a margin of 0, because the distance between positive pairs cannot be negative, and the margin does not contribute to the gradient. So allowing the hyperparameter optimization to explore negative margins was unnecesary.","title":"For the contrastive loss, why is the optimal positive margin a negative value?"},{"location":"papers/mlrc/#optimization-plots","text":"","title":"Optimization plots"}]}