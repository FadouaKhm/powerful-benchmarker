



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.dev0, mkdocs-material-4.6.0">
    
    
      
        <title>Powerful Benchmarker</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/application.1b62728e.css">
      
        <link rel="stylesheet" href="assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="">
      
    
    
      <script src="assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="assets/fonts/material-icons.css">
    
    
    
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="black" data-md-color-accent="black">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#powerful-benchmarker" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="." title="Powerful Benchmarker" class="md-header-nav__button md-logo">
          
            <i class="md-icon"></i>
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Powerful Benchmarker
            </span>
            <span class="md-header-nav__topic">
              
                Home
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/KevinMusgrave/powerful-benchmarker/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/powerful-benchmarker
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="." title="Powerful Benchmarker" class="md-nav__button md-logo">
      
        <i class="md-icon"></i>
      
    </a>
    Powerful Benchmarker
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/KevinMusgrave/powerful-benchmarker/" title="Go to repository" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    KevinMusgrave/powerful-benchmarker
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Home
      </label>
    
    <a href="." title="Home" class="md-nav__link md-nav__link--active">
      Home
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    Usage
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-default-flags" class="md-nav__link">
    Set default flags
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#try-a-basic-command" class="md-nav__link">
    Try a basic command
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#override-config-options-at-the-command-line" class="md-nav__link">
    Override config options at the command line
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#combine-yaml-files-at-the-command-line" class="md-nav__link">
    Combine yaml files at the command line
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    Resume training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reproducing-benchmark-results" class="md-nav__link">
    Reproducing benchmark results
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-options" class="md-nav__link">
    Evaluation options
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#split-schemes-and-cross-validation" class="md-nav__link">
    Split schemes and cross validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta-logs" class="md-nav__link">
    Meta logs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-to-tune-hyperparameters" class="md-nav__link">
    Bayesian optimization to tune hyperparameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register-your-own-classes-and-modules" class="md-nav__link">
    Register your own classes and modules
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="api_parsers/" title="API Parsers" class="md-nav__link">
      API Parsers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="architectures/" title="Architectures" class="md-nav__link">
      Architectures
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      Configs
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        Configs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_dataset/" title="Config Dataset" class="md-nav__link">
      Config Dataset
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_eval/" title="Config Eval" class="md-nav__link">
      Config Eval
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_general/" title="Config General" class="md-nav__link">
      Config General
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_loss_and_miners/" title="Config Loss and Miners" class="md-nav__link">
      Config Loss and Miners
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_models/" title="Config Models" class="md-nav__link">
      Config Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_optimizers/" title="Config Optimizers" class="md-nav__link">
      Config Optimizers
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="configs/config_transforms/" title="Config Transforms" class="md-nav__link">
      Config Transforms
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="datasets/" title="Datasets" class="md-nav__link">
      Datasets
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="runners/" title="Runners" class="md-nav__link">
      Runners
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="split_managers/" title="Split Managers" class="md-nav__link">
      Split Managers
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="utils/" title="Utils" class="md-nav__link">
      Utils
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-9" type="checkbox" id="nav-9">
    
    <label class="md-nav__link" for="nav-9">
      Papers
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-9">
        Papers
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="papers/mlrc/" title="A Metric Learning Reality Check" class="md-nav__link">
      A Metric Learning Reality Check
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    Usage
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-default-flags" class="md-nav__link">
    Set default flags
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#try-a-basic-command" class="md-nav__link">
    Try a basic command
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#override-config-options-at-the-command-line" class="md-nav__link">
    Override config options at the command line
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#combine-yaml-files-at-the-command-line" class="md-nav__link">
    Combine yaml files at the command line
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resume-training" class="md-nav__link">
    Resume training
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reproducing-benchmark-results" class="md-nav__link">
    Reproducing benchmark results
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#evaluation-options" class="md-nav__link">
    Evaluation options
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#split-schemes-and-cross-validation" class="md-nav__link">
    Split schemes and cross validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#meta-logs" class="md-nav__link">
    Meta logs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-optimization-to-tune-hyperparameters" class="md-nav__link">
    Bayesian optimization to tune hyperparameters
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#register-your-own-classes-and-modules" class="md-nav__link">
    Register your own classes and modules
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/KevinMusgrave/powerful-benchmarker/edit/master/docs/index.md" title="Edit this page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="powerful-benchmarker">Powerful Benchmarker<a class="headerlink" href="#powerful-benchmarker" title="Permanent link">&para;</a></h1>
<h2 id="installation">Installation<a class="headerlink" href="#installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="err">pip install powerful-benchmarker</span>
</pre></div>


<h2 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">&para;</a></h2>
<h3 id="set-default-flags">Set default flags<a class="headerlink" href="#set-default-flags" title="Permanent link">&para;</a></h3>
<p>The easiest way to get started is to download the <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py">example script</a>. Then change the default values for the following flags:</p>
<ul>
<li><code>pytorch_home</code> is where you want to save downloaded pretrained models.</li>
<li><code>dataset_root</code> is where your datasets are located.</li>
<li><code>root_experiment_folder</code> is where you want all experiment data to be saved.</li>
</ul>
<h3 id="try-a-basic-command">Try a basic command<a class="headerlink" href="#try-a-basic-command" title="Permanent link">&para;</a></h3>
<p>The following command will run an experiment using the <a href="https://github.com/KevinMusgrave/powerful-benchmarker/tree/master/src/powerful_benchmarker/configs">default config files</a>, as well as download the CUB200 dataset into your <code>dataset_root</code></p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --dataset {CUB200: {download: True}}</span>
</pre></div>


<p>(For the rest of this readme, we'll assume the datasets have already been downloaded.)</p>
<p>Experiment data is saved in the following format:</p>
<div class="codehilite"><pre><span></span><span class="err">&lt;root_experiment_folder&gt;</span>
<span class="err">|-&lt;experiment_name&gt;</span>
<span class="err">  |-configs</span>
<span class="err">    |-config_dataset.yaml</span>
<span class="err">    |-config_eval.yaml</span>
<span class="err">    |-config_general.yaml</span>
<span class="err">    |-config_loss_and_miners.yaml</span>
<span class="err">    |-config_models.yaml</span>
<span class="err">    |-config_optimizers.yaml</span>
<span class="err">    |-config_transforms.yaml</span>
<span class="err">  |-&lt;split scheme name&gt;</span>
<span class="err">    |-saved_models</span>
<span class="err">    |-saved_csvs</span>
<span class="err">    |-tensorboard_logs</span>
<span class="err">  |-meta_logs</span>
<span class="err">    |-saved_csvs</span>
<span class="err">    |-tensorboard_logs</span>
</pre></div>


<h3 id="override-config-options-at-the-command-line">Override config options at the command line<a class="headerlink" href="#override-config-options-at-the-command-line" title="Permanent link">&para;</a></h3>
<p>The default config files use a <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_models/default.yaml">batch size of 32</a>. You can override this default value at the command line:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --batch_size 256</span>
</pre></div>


<p>Complex options (i.e. nested dictionaries) can also be specified at the command line:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py \</span>
<span class="err">--experiment_name test1 \</span>
<span class="err">--mining_funcs {tuple_miner: {PairMarginMiner: {pos_margin: 0.5, neg_margin: 0.5}}}</span>
</pre></div>


<p>The <code>~OVERRIDE~</code> suffix is required to completely override complex config options. For example, the following overrides the <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_loss_and_miners/default.yaml">default loss function</a>:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py \</span>
<span class="err">--experiment_name test1 \</span>
<span class="err">--loss_funcs {metric_loss~OVERRIDE~: {ArcFaceLoss: {margin: 30, scale: 64, embedding_size: 128}}}</span>
</pre></div>


<p>Leave out the <code>~OVERRIDE~</code> suffix if you want to merge options. For example, we can add an optimizer for our loss function's parameters:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py \</span>
<span class="err">--experiment_name test1 \</span>
<span class="err">--optimizers {metric_loss_optimizer: {SGD: {lr: 0.01}}} </span>
</pre></div>


<p>This will be included along with the <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/src/powerful_benchmarker/configs/config_optimizers/default.yaml">default optimizers</a>. </p>
<p>We can change the learning rate of the trunk_optimizer, but keep all other parameters the same:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py \</span>
<span class="err">--experiment_name test1 \</span>
<span class="err">--optimizers {trunk_optimizer: {RMSprop: {lr: 0.01}}} </span>
</pre></div>


<p>Or we can make trunk_optimizer use Adam, but leave embedder_optimizer to the default setting: </p>
<div class="codehilite"><pre><span></span><span class="err">python run.py \</span>
<span class="err">--experiment_name test1 \</span>
<span class="err">--optimizers {trunk_optimizer~OVERRIDE~: {Adam: {lr: 0.01}}} </span>
</pre></div>


<h3 id="combine-yaml-files-at-the-command-line">Combine yaml files at the command line<a class="headerlink" href="#combine-yaml-files-at-the-command-line" title="Permanent link">&para;</a></h3>
<p>The following merges the <code>with_cars196</code> config file into the <code>default</code> config file, in the <code>config_general</code> category.</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --config_general [default, with_cars196]</span>
</pre></div>


<p>This is convenient when you want to change a few settings (specified in <code>with_cars196</code>), and keep all the other options unchanged (specified in <code>default</code>). You can specify any number of config files to merge, and they get loaded and merged in the order that you specify.</p>
<h3 id="resume-training">Resume training<a class="headerlink" href="#resume-training" title="Permanent link">&para;</a></h3>
<p>The following resumes training for the <code>test1</code> experiment, using the latest saved models.</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --resume_training latest</span>
</pre></div>


<p>You can also resume using the model with the best validation accuracy:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --resume_training best</span>
</pre></div>


<p>Let's say you finished training for 100 epochs, and decide you want to train for another 50 epochs, for a total of 150. You would run:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --experiment_name test1 --resume_training latest \</span>
<span class="err">--num_epochs_train 150 --merge_argparse_when_resuming</span>
</pre></div>


<p>(The <code>merge_argparse_when_resuming</code> tells the code that you want to make changes to the original experiment configuration. If you don't use this flag, then the code will ignore your command line arguments, and use the original configuration. The purpose of this is to avoid accidentally changing configs in the middle of an experiment.)</p>
<p>Now in your experiments folder you'll see the original config files, and a new folder starting with <code>resume_training</code>.</p>
<div class="codehilite"><pre><span></span><span class="err">&lt;root_experiment_folder&gt;</span>
<span class="err">|-&lt;experiment_name&gt;</span>
<span class="err">  |-configs</span>
<span class="err">    |-config_eval.yaml</span>
<span class="err">    ...</span>
<span class="err">    |-resume_training_config_diffs_&lt;underscore delimited numbers&gt;</span>
<span class="err">  ...</span>
</pre></div>


<p>This folder contains all differences between the originally saved config files and the parameters that you've specified at the command line. In this particular case, there should just be a single file <code>config_general.yaml</code> with a single line: <code>num_epochs_train: 150</code>. </p>
<p>The underscore delimited numbers in the folder name indicate which models were loaded for each <a href="#split-schemes-and-cross-validation">split scheme</a>. For example, let's say you are doing cross validation with 3 folds. The training process has finished 50, 30, and 0 epochs of folds 0, 1, and 2, respectively. You decide to stop training, and resume training with a different batch size. Now the config diff folder will be named <code>resume_training_config_diffs_50_30_0</code>.</p>
<h3 id="reproducing-benchmark-results">Reproducing benchmark results<a class="headerlink" href="#reproducing-benchmark-results" title="Permanent link">&para;</a></h3>
<p>To reproduce an experiment from the benchmark spreadsheets, use the <code>--reproduce_results</code> flag:
1. In the benchmark spreadsheet, click on the google drive link under the "config files" column.
2. Download the folders you want (for example <code>cub200_old_approach_triplet_batch_all</code>), into some folder on your computer. For example, I downloaded into <code>/home/experiments_to_reproduce</code>
3. Then run:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \</span>
<span class="err">--experiment_name cub200_old_approach_triplet_batch_all_reproduced</span>
</pre></div>


<p>If you'd like to change some parameters when reproducing results, you can either make those changes in the config files, or at the command line. For example, maybe you'd like to change the number of dataloaders:</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --reproduce_results /home/experiments_to_reproduce/cub200_old_approach_triplet_batch_all \</span>
<span class="err">--experiment_name cub200_old_approach_triplet_batch_all_reproduced \</span>
<span class="err">--dataloader_num_workers 16 \</span>
<span class="err">--eval_dataloader_num_workers 16 \</span>
<span class="err">--merge_argparse_when_resuming</span>
</pre></div>


<p>The <code>merge_argparse_when_resuming</code> flag is required in order to use a different configuration from the one in the <code>reproduce_results</code> folder.</p>
<h3 id="evaluation-options">Evaluation options<a class="headerlink" href="#evaluation-options" title="Permanent link">&para;</a></h3>
<p>By default, your model will be saved and evaluated on the validation set every <code>save_interval</code> epochs.</p>
<p>To get accuracy for specific splits, use the <code>--splits_to_eval</code> flag and pass in a python-style list of split names. For example <code>--splits_to_eval [train, test]</code></p>
<p>To run evaluation only, use the <code>--evaluate</code> flag.</p>
<h3 id="split-schemes-and-cross-validation">Split schemes and cross validation<a class="headerlink" href="#split-schemes-and-cross-validation" title="Permanent link">&para;</a></h3>
<p>One weakness of many metric-learning papers is that they have been training and testing on the same handful of datasets for years. They have also been splitting data into a 50/50 train/test split scheme, instead of train/val/test. This has likely lead to overfitting on the "test" set, as people have tuned hyperparameters and created algorithms with direct feedback from the "test" set.</p>
<p>To remedy this situation, this benchmarker allows the user to specify the split scheme. Here's an example config:</p>
<div class="codehilite"><pre><span></span><span class="nt">split_manager</span><span class="p">:</span>
  <span class="nt">ClassDisjointSplitManager</span><span class="p">:</span>
    <span class="nt">test_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.5</span>
    <span class="nt">test_start_idx</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.5</span>
    <span class="nt">num_training_partitions</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
    <span class="nt">num_training_sets</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
</pre></div>


<p>Translation:
- The test set consists of classes with labels in <code>[num_labels * test_start_idx, num_labels * (test_start_idx + test_size)]</code>. Note that if we set <code>test_start_idx</code> to 0.9, the range would wrap around to the beginning (0.9 to 1, 0 to 0.4). 
- The remaining classes will be split into 10 equal sized partitions. 
- 5 of those partitions will be used for training. In other words, 5-fold cross validation will be performed, but the size of the partitions will be the same as if 10-fold cross validation was being performed.</p>
<p>When evaluating the cross-validated models, the best model from each fold will be loaded, and the results be averaged. Alternatively, you can set the config option <code>meta_testing_method</code> to <code>meta_ConcatenateEmbeddings</code>. This will load the best model from each fold, but treat them as one model during evaluation on the test set, by concatenating their outputs.</p>
<h3 id="meta-logs">Meta logs<a class="headerlink" href="#meta-logs" title="Permanent link">&para;</a></h3>
<p>When doing cross validation, a new set of meta records will be created. The meta records show the average of the best accuracies of your training runs. You can find these records on tensorboard and in the meta_logs folder.</p>
<h3 id="bayesian-optimization-to-tune-hyperparameters">Bayesian optimization to tune hyperparameters<a class="headerlink" href="#bayesian-optimization-to-tune-hyperparameters" title="Permanent link">&para;</a></h3>
<p>You can use bayesian optimization using the same <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py">example script</a>. In your config files or at the command line, append <code>~BAYESIAN~</code> to any parameter that you want to tune, followed by a lower and upper bound in square brackets. If your parameter operates on a log scale (for example, learning rates), then append <code>~LOG_BAYESIAN~</code>. You must also specify the number of iterations with the <code>--bayes_opt_iters</code> command line flag.</p>
<p>Here is an example script which uses bayesian optimization to tune 3 hyperparameters for the multi similarity loss.</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --bayes_opt_iters 50 \</span>
<span class="err">--loss_funcs~OVERRIDE~ {metric_loss: {MultiSimilarityLoss: {alpha~LOG_BAYESIAN~: [0.01, 100], beta~LOG_BAYESIAN~: [0.01, 100], base~BAYESIAN~: [0, 1]}}} \</span>
<span class="err">--experiment_name cub_bayes_opt \</span>
</pre></div>


<p>If you stop and want to resume bayesian optimization, simply use <code>run.py</code> with the same <code>experiment_name</code> you were using before. </p>
<p>You can change the optimization bounds when resuming, by either changing the bounds in your config files or at the command line. If you're using the command line, make sure to also use the <code>--merge_argparse_when_resuming</code> flag.</p>
<p>You can also run a number of reproductions for the best parameters, so that you can obtain a confidence interval for your results. Use the <code>reproductions</code> flag, and pass in the number of reproductions you want to perform at the end of bayesian optimization.</p>
<div class="codehilite"><pre><span></span><span class="err">python run.py --bayes_opt_iters 50 --reproductions 10 \</span>
<span class="err">--experiment_name cub_bayes_opt \</span>
</pre></div>


<h3 id="register-your-own-classes-and-modules">Register your own classes and modules<a class="headerlink" href="#register-your-own-classes-and-modules" title="Permanent link">&para;</a></h3>
<p>By default, the API gives you access to losses/miners/datasets/optimizers/schedulers/trainers etc that are available in powerful-benchmarker, PyTorch, and pytorch-metric-learning.</p>
<p>Let's say you make your own loss and mining functions, and you'd like to have access to them via the API. You can accomplish this by replacing the last two lines of the <a href="https://github.com/KevinMusgrave/powerful-benchmarker/blob/master/examples/run.py">example script</a> with this:</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">losses</span><span class="p">,</span> <span class="n">miners</span>

<span class="c1"># your custom loss function</span>
<span class="k">class</span> <span class="nc">YourLossFunction</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">BaseMetricLossFunction</span><span class="p">):</span>
   <span class="o">...</span>

<span class="c1"># your custom mining function</span>
<span class="k">class</span> <span class="nc">YourMiningFunction</span><span class="p">(</span><span class="n">miners</span><span class="o">.</span><span class="n">BaseTupleMiner</span><span class="p">):</span>
   <span class="o">...</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">runner</span><span class="p">(</span><span class="o">**</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">))</span>

<span class="c1"># make the runner aware of them</span>
<span class="n">r</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">YourLossFunction</span><span class="p">)</span>
<span class="n">r</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;miner&quot;</span><span class="p">,</span> <span class="n">YourMiningFunction</span><span class="p">)</span>
<span class="n">r</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>


<p>Now you can access your custom classes just like any other class:</p>
<div class="codehilite"><pre><span></span><span class="nt">loss_funcs</span><span class="p">:</span>
  <span class="nt">metric_loss</span><span class="p">:</span> 
    <span class="nt">YourLossFunction</span><span class="p">:</span>

<span class="nt">mining_funcs</span><span class="p">:</span>
  <span class="nt">tuple_miner</span><span class="p">:</span>
    <span class="nt">YourMiningFunction</span><span class="p">:</span>
</pre></div>


<p>If you have a module containing multiple classes and you want to register all those classes, you can simply register the module:</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">YourModuleOfLosses</span>
<span class="n">r</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">YourModuleOfLosses</span><span class="p">)</span>
</pre></div>


<p>Registering your own trainer is a bit more involved, because you need to also create an associated API parser. The name of the api parser should be <code>APIParser&lt;name of your training method&gt;</code>. </p>
<p>Here's an example where I make a trainer that extends <code>trainers.MetricLossOnly</code>, and takes in an additional argument <code>foo</code>. In order to pass this in, the API parser needs to add <code>foo</code> to the trainer kwargs, and this is done in the <code>get_trainer_kwargs</code> method.</p>
<div class="codehilite"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_metric_learning</span> <span class="kn">import</span> <span class="n">trainers</span>
<span class="kn">from</span> <span class="nn">powerful_benchmarker</span> <span class="kn">import</span> <span class="n">api_parsers</span>

<span class="k">class</span> <span class="nc">YourTrainer</span><span class="p">(</span><span class="n">trainers</span><span class="o">.</span><span class="n">MetricLossOnly</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">foo</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">foo</span> <span class="o">=</span> <span class="n">foo</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;foo = &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">foo</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">APIYourTrainer</span><span class="p">(</span><span class="n">api_parsers</span><span class="o">.</span><span class="n">BaseAPIParser</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">get_foo</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;hello&quot;</span>

    <span class="k">def</span> <span class="nf">get_trainer_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">trainer_kwargs</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_trainer_kwargs</span><span class="p">()</span>
        <span class="n">trainer_kwargs</span><span class="p">[</span><span class="s2">&quot;foo&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_foo</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">trainer_kwargs</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">runner</span><span class="p">(</span><span class="o">**</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">))</span>
<span class="n">r</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;trainer&quot;</span><span class="p">,</span> <span class="n">YourTrainer</span><span class="p">)</span>
<span class="n">r</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">&quot;api_parser&quot;</span><span class="p">,</span> <span class="n">APIYourTrainer</span><span class="p">)</span>
<span class="n">r</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
        
          <a href="api_parsers/" title="API Parsers" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                API Parsers
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="assets/javascripts/application.808e90bb.js"></script>
      
      <script>app.initialize({version:"1.1.dev0",url:{base:"."}})</script>
      
    
  </body>
</html>